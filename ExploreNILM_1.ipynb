{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExploreNILM_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BitKnitting/transfer_nilm_exploration/blob/master/ExploreNILM_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSMMkEhscci3",
        "colab_type": "text"
      },
      "source": [
        "# Overview\n",
        "I am self-teaching myself Deep Learning methods using TensorFlow 2.0 and Keras.  I wanted to learn by applying the Deep Learning methods to a scenario that interests me.  The scenario I chose was figuring out if a microwave was on within a house's aggregated electricity readings.  This field of research is called NILM - Non-Intrusive Load Monitoring.  It's called this because it takes readings from how we typically measure a house's electricity - all appliances captured together - and disaggregates them into the appliances that used the electricity.  Appliances typically are microwaves, washing and drying machines, heaters, air conditioners, etc.\n",
        "\n",
        "## Specific Scenario\n",
        "To accomplish my goal of getting predictions from input data fed into a Deep Learning model, I need to identify:\n",
        "- The Deep Learning model\n",
        "- The input data\n",
        "\n",
        "to be used.\n",
        "\n",
        "### Deep Learning Model\n",
        "I chose to replicate the Deep Learning model written about in the document [ _Transfer Learning for Non-Intrusive Load Monitoring_ ](https://arxiv.org/abs/1902.08835). Figure 1 on page 4 captures the Deep Learning model used in their research:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1fkgxjqm0UzTKisCIyKJV9BNQhuAiBt6E)  \n",
        "\n",
        "The researchers call this _seq2point Learning_ because the input is 599 samples of aggregate electricity readings and the output is the midpoint of the target appliance (in this case a microwave).\n",
        "\n",
        "### Input Data\n",
        "\n",
        "Section IV in [the research paper](https://arxiv.org/abs/1902.08835) discusses the Data sets.\n",
        "\n",
        "I decided to use the REDD dataset.\n",
        "#### Building the Input Data\n",
        "The researchers kindly provided [a GitHub repo with their code](https://github.com/MingjunZhong/transferNILM).  I used:\n",
        "- a modified version of [`create_trainset_redd.py`](https://github.com/MingjunZhong/transferNILM/blob/master/dataset_management/redd/create_trainset_redd.py).  The code uses the parameters file to get the data out of the other researcher's files stored on the internet.  It creates a training, validation, and test data sets that have been normalized using the method the researchers discuss in their paper.\n",
        "- the parameters provided in [`redd_parameters.py`](https://github.com/MingjunZhong/transferNILM/blob/master/dataset_management/redd/redd_parameters.py).\n",
        "\n",
        "My [modified version of `create_trainset_redd.py`](https://github.com/BitKnitting/transfer_nilm_exploration/blob/master/code/create_trainset_redd.py) creates [three files](https://github.com/BitKnitting/transfer_nilm_exploration/tree/master/created_data/REDD/microwave) - a training, validation, and test file.  \n",
        "\n",
        "The files are saved as zipped pickle files because I find this format to be the easiest and fastest way of loading these type of files into Pandas.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fA8OByNzrChu",
        "colab_type": "text"
      },
      "source": [
        "# Load The Data\n",
        "I wrote a python file - [ `ziptodf.py` ](https://github.com/BitKnitting/transfer_nilm_exploration/blob/master/code/python_lib/ziptodf.py) - that loads zipped pickle files and then can show basics stats about the file.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz1szLFnpGlE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "5e1c58b1-564c-483a-f9ae-80a8090776c9"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/BitKnitting/transfer_nilm_exploration/master/code/python_lib/ziptodf.py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-08 13:14:34--  https://raw.githubusercontent.com/BitKnitting/transfer_nilm_exploration/master/code/python_lib/ziptodf.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1277 (1.2K) [text/plain]\n",
            "Saving to: ‘ziptodf.py’\n",
            "\n",
            "ziptodf.py          100%[===================>]   1.25K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-12-08 13:14:39 (278 MB/s) - ‘ziptodf.py’ saved [1277/1277]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao0CATpaXCak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ziptodf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcpLINX7XEHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = ziptodf.get_dataframe_from_pkl_zip('https://raw.githubusercontent.com/BitKnitting/transfer_nilm_exploration/master/created_data/REDD/microwave/microwave_training_.pkl.zip')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jz6mxtV5XV4T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        },
        "outputId": "cc497339-5f26-44e4-bd95-2e2bc917a2b2"
      },
      "source": [
        "ziptodf.print_stats(df,'REDD Microwave training data')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "REDD Microwave training data\n",
            "**************************\n",
            "Start index: 0\n",
            "--------------------------\n",
            "End index: 300350\n",
            "--------------------------\n",
            "Shape: (300351, 2)\n",
            "--------------------------\n",
            "Data types: \n",
            "aggregate    float64\n",
            "microwave    float64\n",
            "dtype: object\n",
            "--------------------------\n",
            "Number of missing values:\n",
            "aggregate    0\n",
            "microwave    0\n",
            "dtype: int64\n",
            "--------------------------\n",
            "Summary Stats:\n",
            "            aggregate      microwave\n",
            "count  300351.000000  300351.000000\n",
            "mean       -0.208512      -0.610165\n",
            "std         0.736927       0.130792\n",
            "min        -0.601689      -0.625000\n",
            "25%        -0.489411      -0.622500\n",
            "50%        -0.329694      -0.620000\n",
            "75%        -0.239884      -0.619167\n",
            "max         8.755021       1.848750\n",
            "--------------------------\n",
            "Head:\n",
            "    aggregate  microwave\n",
            "0  -0.594218  -0.620000\n",
            "1  -0.594218  -0.620000\n",
            "2  -0.594211  -0.619583\n",
            "3  -0.594240  -0.620000\n",
            "4  -0.594295  -0.620000\n",
            "--------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CB4NxaPljZmr",
        "colab_type": "text"
      },
      "source": [
        "The dataframe has two columns. The first column ('aggregate') are the normalized aggregated electricity readings from the houses measured during the REDD project.  This is the input data.  The second column ('microwave') are the microwave's electricity readings.  This is the target data.\n",
        "\n",
        "The researchers specify a window size of 599.  I will set the input_shape=(599,1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPkSIwctfbcD",
        "colab_type": "text"
      },
      "source": [
        "# Get Data Ready for the Deep Learning Model\n",
        "\n",
        "I'll start  by loading TensorFlow (2.0). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2wW_MSaiUql",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2991c1dd-8831-4969-aba0-c35f10fe3ccc"
      },
      "source": [
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYtzWDT8mp5_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((df['aggregate'].values, df['microwave'].values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnKKmONihfbD",
        "colab_type": "text"
      },
      "source": [
        "# Make the Model\n",
        "The researchers note in their paper: _...a fixed-length window of aggregate active power consumption signal is given as input. For the neural networks employed\n",
        "in this paper, a sample window has 599 data points...In our experiments,\n",
        "a sample window was generated by sliding the window forward by a\n",
        "single data point, and so all the possible sample windows were used\n",
        "for training. The windows of the mains were used as the inputs to the\n",
        "6 neural networks, whilst the midpoints of the corresponding windows\n",
        "of the appliances were used as the targets._\n",
        "  \n",
        "My goal is to replicate the model used in [`cnnModel.py`](https://github.com/MingjunZhong/transferNILM/blob/master/cnnModel.py).  The model uses a more advanced functional approach to creating a model than my current knowledge affords.  My goal is to replicate the model as much as possible using Keras's `Sequential` approach.  \n",
        "Here is a copy of the model within `cnnModel.py`:  \n",
        "```\n",
        "def get_model(appliance, input_tensor, window_length, transfer_dense=False, transfer_cnn=False,\n",
        "              cnn='kettle', n_dense=1, pretrainedmodel_dir='./models/'):\n",
        "\n",
        "    reshape = Reshape((-1, window_length, 1),\n",
        "                      )(input_tensor)\n",
        "\n",
        "    cnn1 = Conv2D(filters=30,\n",
        "                  kernel_size=(10, 1),\n",
        "                  strides=(1, 1),\n",
        "                  padding='same',\n",
        "                  activation='relu',\n",
        "                  )(reshape)\n",
        "\n",
        "    cnn2 = Conv2D(filters=30,\n",
        "                  kernel_size=(8, 1),\n",
        "                  strides=(1, 1),\n",
        "                  padding='same',\n",
        "                  activation='relu',\n",
        "                  )(cnn1)\n",
        "\n",
        "    cnn3 = Conv2D(filters=40,\n",
        "                  kernel_size=(6, 1),\n",
        "                  strides=(1, 1),\n",
        "                  padding='same',\n",
        "                  activation='relu',\n",
        "                  )(cnn2)\n",
        "\n",
        "    cnn4 = Conv2D(filters=50,\n",
        "                  kernel_size=(5, 1),\n",
        "                  strides=(1, 1),\n",
        "                  padding='same',\n",
        "                  activation='relu',\n",
        "                  )(cnn3)\n",
        "\n",
        "    cnn5 = Conv2D(filters=50,\n",
        "                  kernel_size=(5, 1),\n",
        "                  strides=(1, 1),\n",
        "                  padding='same',\n",
        "                  activation='relu',\n",
        "                  )(cnn4)\n",
        "\n",
        "    flat = Flatten(name='flatten')(cnn5)\n",
        "\n",
        "    d = Dense(1024, activation='relu', name='dense')(flat)\n",
        "\n",
        "    if n_dense == 1:\n",
        "        label = d\n",
        "    elif n_dense == 2:\n",
        "        d1 = Dense(1024, activation='relu', name='dense1')(d)\n",
        "        label = d1\n",
        "    elif n_dense == 3:\n",
        "        d1 = Dense(1024, activation='relu', name='dense1')(d)\n",
        "        d2 = Dense(1024, activation='relu', name='dense2')(d1)\n",
        "        label = d2\n",
        "\n",
        "    d_out = Dense(1, activation='linear', name='output')(label)\n",
        "\n",
        "    model = Model(inputs=input_tensor, outputs=d_out)\n",
        "\n",
        "    session = K.get_session()\n",
        "\n",
        "    if transfer_dense:\n",
        "        log(\"Transfer learning...\")\n",
        "        log(\"...loading an entire pre-trained model\")\n",
        "        weights_loader(model, pretrainedmodel_dir+'/cnn_s2p_' + appliance + '_pointnet_model')\n",
        "        model_def = model\n",
        "    elif transfer_cnn and not transfer_dense:\n",
        "        log(\"Transfer learning...\")\n",
        "        log('...loading a ' + appliance + ' pre-trained-cnn')\n",
        "        cnn_weights_loader(model, cnn, pretrainedmodel_dir)\n",
        "        model_def = model\n",
        "        for idx, layer1 in enumerate(model_def.layers):\n",
        "            if hasattr(layer1, 'kernel_initializer') and 'conv2d' not in layer1.name and 'cnn' not in layer1.name:\n",
        "                log('Re-initialize: {}'.format(layer1.name))\n",
        "                layer1.kernel.initializer.run(session=session)\n",
        "\n",
        "    elif not transfer_dense and not transfer_cnn:\n",
        "        log(\"Standard training...\")\n",
        "        log(\"...creating a new model.\")\n",
        "        model_def = model\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Model selection error.')\n",
        "\n",
        "    # Printing, logging and plotting the model\n",
        "    print_summary(model_def)\n",
        "    # plot_model(model, to_file='./model.png', show_shapes=True, show_layer_names=True, rankdir='TB')\n",
        "\n",
        "    # Adding network structure to both the log file and output terminal\n",
        "    files = [x for x in os.listdir('./') if x.endswith(\".log\")]\n",
        "    with open(max(files, key=os.path.getctime), 'a') as fh:\n",
        "        # Pass the file handle in as a lambda function to make it callable\n",
        "        model_def.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
        "\n",
        "    # Check weights slice\n",
        "    for v in tf.trainable_variables():\n",
        "        if v.name == 'conv2d_1/kernel:0':\n",
        "            cnn1_weights = session.run(v)\n",
        "    return model_def, cnn1_weights\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh9CA16_jOYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Conv2D,Flatten,Dense,Reshape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD-tnMisidYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Reshape((-1,599,1)))\n",
        "model.add(Conv2D(filters=30,kernel_size=(10,1),strides=(1,1),padding='same',activation='relu',input_shape=(599,1)))\n",
        "model.add(Conv2D(filters=30,kernel_size=(8,1),strides=(1,1),padding='same',activation='relu'))\n",
        "model.add(Conv2D(filters=40,kernel_size=(6,1),strides=(1,1),padding='same',activation='relu'))\n",
        "model.add(Conv2D(filters=50,kernel_size=(5,1),strides=(1,1),padding='same',activation='relu'))\n",
        "model.add(Conv2D(filters=50,kernel_size=(5,1),strides=(1,1),padding='same',activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024,activation='relu'))\n",
        "model.add(Dense(1, activation='linear', name='output'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoLZM9yz2Pq1",
        "colab_type": "text"
      },
      "source": [
        "# Compile the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POsekRVr2xlH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i14jfaek4EBS",
        "colab_type": "text"
      },
      "source": [
        "# Fit the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7M0EgSI6be5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "bbeead82-c719-4338-8056-348d6a9dc2b7"
      },
      "source": [
        "model.fit(dataset, epochs=10)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer sequential_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
            "\n",
            "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
            "\n",
            "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-f8c5e0c71664>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    548\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \u001b[0;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2419\u001b[0;31m       \u001b[0mall_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_model_with_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2420\u001b[0m       \u001b[0mis_build_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_build_model_with_inputs\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m   2620\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2621\u001b[0m       \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2622\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2623\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_dict_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[0;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[1;32m   2707\u001b[0m           \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2708\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2709\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2710\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2711\u001b[0m         \u001b[0;31m# This Model or a submodel is dynamic and hasn't overridden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    840\u001b[0m                     not base_layer_utils.is_in_eager_or_tf_function()):\n\u001b[1;32m    841\u001b[0m                   \u001b[0;32mwith\u001b[0m \u001b[0mauto_control_deps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAutomaticControlDependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0macd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m                     \u001b[0;31m# Wrap Tensors in `outputs` in `tf.identity` to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m                     \u001b[0;31m# circular dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0;31m# `outputs` will be the inputs to the next layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2139\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2140\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2141\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2142\u001b[0m       \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2143\u001b[0m       \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.0.0/python3.6/tensorflow_core/python/keras/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m       raise ValueError('The last dimension of the inputs to `Dense` '\n\u001b[0m\u001b[1;32m   1016\u001b[0m                        'should be defined. Found `None`.')\n\u001b[1;32m   1017\u001b[0m     \u001b[0mlast_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The last dimension of the inputs to `Dense` should be defined. Found `None`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSikubli7UBr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "3b253d60-a753-48fd-ad04-3235a398b82d"
      },
      "source": [
        "import numpy as np\n",
        "offset = int(0.5*(599-1.0))\n",
        "display(offset)\n",
        "max_batchsize = 33372 - 2 * offset\n",
        "display(max_batchsize)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "299"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "32774"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m57ECWBB_vs3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a85521ad-075f-44d5-d8b9-8fecd46cf6aa"
      },
      "source": [
        "np.random.shuffle(skip_idx)\n",
        "skip_idx"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 9., 23., 20., 19., 11.,  6.,  8., 12., 27., 17., 10.,  3., 26.,\n",
              "       25.,  4., 14.,  5., 15., 18., 24.,  1.,  7.,  2.,  0., 16., 29.,\n",
              "       22., 21., 28., 13.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0BYkPWACeSS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d849d28f-982f-4ca2-dd6a-de7571da75ca"
      },
      "source": [
        "  indices = np.arange(max_batchsize)\n",
        "  print('indices prior to shuffling: {}'.format(indices))\n",
        "  np.random.shuffle(indices)\n",
        "  print('indices after shuffling: {}'.format(indices))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "indices prior to shuffling: [    0     1     2 ... 32771 32772 32773]\n",
            "indices after shuffling: [15694 11232  7599 ... 21021 21856  2652]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGmeqzbPASg-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d581fa5f-7713-4007-d6f7-4aafd833bf98"
      },
      "source": [
        "np.random.shuffle(arr)\n",
        "arr"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0qWEf6mDth-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "f6e88bff-ad37-4222-8971-c6cd5cfb918a"
      },
      "source": [
        "\n",
        "# Batch before shuffle.\n",
        "dataset = tf.data.Dataset.from_tensor_slices([0, 0, 0, 1, 1, 1, 2, 2, 2, 3])\n",
        "dataset = dataset.batch(2)\n",
        "dataset = dataset.shuffle(10)\n",
        "\n",
        "for elem in dataset:\n",
        "  print(elem)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 1], shape=(2,), dtype=int32)\n",
            "tf.Tensor([0 0], shape=(2,), dtype=int32)\n",
            "tf.Tensor([2 2], shape=(2,), dtype=int32)\n",
            "tf.Tensor([2 3], shape=(2,), dtype=int32)\n",
            "tf.Tensor([1 1], shape=(2,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUaxRxW7D91L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "4070a848-5494-47ac-9f36-27d901f9ad36"
      },
      "source": [
        "\n",
        "# Shuffle before batch.\n",
        "dataset = tf.data.Dataset.from_tensor_slices([0, 0, 0, 1, 1, 1, 2, 2, 2])\n",
        "dataset = dataset.shuffle(9)\n",
        "dataset = dataset.batch(3)\n",
        "\n",
        "for elem in dataset:\n",
        "  print(elem)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([1 1 0], shape=(3,), dtype=int32)\n",
            "tf.Tensor([2 2 0], shape=(3,), dtype=int32)\n",
            "tf.Tensor([1 2 0], shape=(3,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKjHe3sT_gk6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8cae0a21-d487-48d2-cc20-aabcb86bdf37"
      },
      "source": [
        "help(np.arange)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on built-in function arange in module numpy:\n",
            "\n",
            "arange(...)\n",
            "    arange([start,] stop[, step,], dtype=None)\n",
            "    \n",
            "    Return evenly spaced values within a given interval.\n",
            "    \n",
            "    Values are generated within the half-open interval ``[start, stop)``\n",
            "    (in other words, the interval including `start` but excluding `stop`).\n",
            "    For integer arguments the function is equivalent to the Python built-in\n",
            "    `range` function, but returns an ndarray rather than a list.\n",
            "    \n",
            "    When using a non-integer step, such as 0.1, the results will often not\n",
            "    be consistent.  It is better to use `numpy.linspace` for these cases.\n",
            "    \n",
            "    Parameters\n",
            "    ----------\n",
            "    start : number, optional\n",
            "        Start of interval.  The interval includes this value.  The default\n",
            "        start value is 0.\n",
            "    stop : number\n",
            "        End of interval.  The interval does not include this value, except\n",
            "        in some cases where `step` is not an integer and floating point\n",
            "        round-off affects the length of `out`.\n",
            "    step : number, optional\n",
            "        Spacing between values.  For any output `out`, this is the distance\n",
            "        between two adjacent values, ``out[i+1] - out[i]``.  The default\n",
            "        step size is 1.  If `step` is specified as a position argument,\n",
            "        `start` must also be given.\n",
            "    dtype : dtype\n",
            "        The type of the output array.  If `dtype` is not given, infer the data\n",
            "        type from the other input arguments.\n",
            "    \n",
            "    Returns\n",
            "    -------\n",
            "    arange : ndarray\n",
            "        Array of evenly spaced values.\n",
            "    \n",
            "        For floating point arguments, the length of the result is\n",
            "        ``ceil((stop - start)/step)``.  Because of floating point overflow,\n",
            "        this rule may result in the last element of `out` being greater\n",
            "        than `stop`.\n",
            "    \n",
            "    See Also\n",
            "    --------\n",
            "    linspace : Evenly spaced numbers with careful handling of endpoints.\n",
            "    ogrid: Arrays of evenly spaced numbers in N-dimensions.\n",
            "    mgrid: Grid-shaped arrays of evenly spaced numbers in N-dimensions.\n",
            "    \n",
            "    Examples\n",
            "    --------\n",
            "    >>> np.arange(3)\n",
            "    array([0, 1, 2])\n",
            "    >>> np.arange(3.0)\n",
            "    array([ 0.,  1.,  2.])\n",
            "    >>> np.arange(3,7)\n",
            "    array([3, 4, 5, 6])\n",
            "    >>> np.arange(3,7,2)\n",
            "    array([3, 5])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AiRDoav_q0M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1e46fe74-49ac-4cbe-9051-322a2ced4d97"
      },
      "source": [
        "indices = np.arange(max_batchsize)\n",
        "indices"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    0,     1,     2, ..., 32771, 32772, 32773])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9o5GzL6rD5F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "outputId": "e4815230-22e5-454b-aa59-b3ecf1354bf3"
      },
      "source": [
        "for start_idx in range(0, max_batchsize, 1000):\n",
        "    excerpt = indices[start_idx:start_idx + 1000]\n",
        "    display(start_idx)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "2000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "3000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "4000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "5000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "6000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "7000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "8000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "9000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "11000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "12000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "13000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "14000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "15000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "16000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "17000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "18000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "19000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "21000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "22000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "23000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "24000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "26000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "27000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "28000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "29000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "31000"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "32000"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GYJTTATrUny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}